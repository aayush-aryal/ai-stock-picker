{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&P 500 Data Fetching and Feature Engineering Pipeline\n",
    "This notebook fetches historical data for all S&P 500 companies and the S&P 500 index, engineers a comprehensive set of technical indicators and features, and saves the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "Import all necessary libraries for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper and Data Fetching Functions\n",
    "These functions handle the core tasks of fetching tickers and downloading data from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_slickcharts_sp500() -> pd.DataFrame:\n",
    "    \"\"\"Scrapes the current list of S&P 500 tickers from SlickCharts.\"\"\"\n",
    "    url = 'https://www.slickcharts.com/sp500'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    df = pd.read_html(io.StringIO(response.text), match='Symbol', index_col='Symbol')[0]\n",
    "    return df.reset_index()\n",
    "\n",
    "def fix_yfinance_indexes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Flattens the multi-level index returned by yfinance for grouped downloads.\"\"\"\n",
    "    return df.stack(level=0, future_stack=True).rename_axis([\"Date\", \"Ticker\"]).reset_index(level=1)\n",
    "\n",
    "def fetch_sp500_stock_data(tickers: list, start_date=\"2003-01-01\") -> pd.DataFrame:\n",
    "    \"\"\"Downloads historical data for a list of tickers in batches.\"\"\"\n",
    "    normalized_tickers = [t.replace(\".\", \"-\") for t in tickers]\n",
    "    all_dfs = []\n",
    "    # Download in batches of 10 to be robust\n",
    "    for i in range(0, len(normalized_tickers), 10):\n",
    "        subset_tickers = normalized_tickers[i:i+10]\n",
    "        data = yf.download(\n",
    "            subset_tickers, \n",
    "            start=start_date, \n",
    "            end=datetime.now(), \n",
    "            group_by=\"Ticker\", \n",
    "            auto_adjust=False\n",
    "        )\n",
    "        if not data.empty:\n",
    "            df_subset = fix_yfinance_indexes(data).reset_index()\n",
    "            all_dfs.append(df_subset)\n",
    "            \n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df = final_df.dropna(subset=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], how=\"all\")\n",
    "    return final_df\n",
    "\n",
    "def fetch_sp500_index_data(start_date=\"2003-01-01\") -> pd.DataFrame:\n",
    "    \"\"\"Fetches historical data for the S&P 500 index (^GSPC).\"\"\"\n",
    "    data = yf.download(\"^GSPC\", start=start_date, end=datetime.now(), auto_adjust=False)\n",
    "    data=data.droplevel(\"Ticker\",axis=1)\n",
    "    data.reset_index()\n",
    "    data.columns.name=(None)\n",
    "\n",
    "    df = data.reset_index()\n",
    "    # Rename columns to be specific to the index\n",
    "    df.columns = [f\"sp500_{x}\" for x in df.columns.tolist()]\n",
    "    \n",
    "    df.rename(columns={\"sp500_Date\": \"Date\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering Functions\n",
    "These functions calculate all the technical indicators and date-based features for both individual stocks and the S&P 500 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rsi(x: pd.Series, period: int = 14) -> pd.Series:\n",
    "    \"\"\"Helper function to calculate Relative Strength Index (RSI).\"\"\"\n",
    "    change = x.diff()\n",
    "    gain = change.where(change > 0, 0.0)\n",
    "    loss = -change.where(change < 0, 0.0)\n",
    "    avg_gain = gain.ewm(span=period, min_periods=period).mean()\n",
    "    avg_loss = loss.ewm(span=period, min_periods=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applies a comprehensive set of feature engineering steps to the stock data.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"Date\"] = pd.to_datetime(df_copy[\"Date\"])\n",
    "    df_copy = df_copy.sort_values([\"Ticker\", \"Date\"])\n",
    "    \n",
    "    # Returns & Moving Averages\n",
    "    df_copy[\"return_1d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].pct_change(1)\n",
    "    df_copy[\"return_5d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].pct_change(5)\n",
    "    df_copy[\"return_20d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].pct_change(20)\n",
    "    df_copy[\"ma_5d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].rolling(5).mean().reset_index(0, drop=True)\n",
    "    df_copy[\"ma_20d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].rolling(20).mean().reset_index(0, drop=True)\n",
    "    df_copy[\"ema_5d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].transform(lambda x: x.ewm(span=5, adjust=False).mean())\n",
    "    df_copy[\"ema_20d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].transform(lambda x: x.ewm(span=20, adjust=False).mean())\n",
    "\n",
    "    # Volatility\n",
    "    df_copy[\"rv_5d\"] = df_copy.groupby(\"Ticker\")[\"return_1d\"].rolling(5).std().reset_index(0, drop=True)\n",
    "    df_copy[\"rv_20d\"] = df_copy.groupby(\"Ticker\")[\"return_1d\"].rolling(20).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # Date Features\n",
    "    df_copy[\"week_of_year\"] = df_copy[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "    df_copy[\"month\"] = df_copy[\"Date\"].dt.month\n",
    "    df_copy[\"quarter\"] = df_copy[\"Date\"].dt.quarter\n",
    "    df_copy[\"day_of_week\"] = df_copy[\"Date\"].dt.dayofweek\n",
    "    df_copy[\"day_of_month\"] = df_copy[\"Date\"].dt.day\n",
    "    df_copy[\"is_quarter_start\"] = df_copy[\"Date\"].dt.is_quarter_start.astype(int)\n",
    "    df_copy[\"is_quarter_end\"] = df_copy[\"Date\"].dt.is_quarter_end.astype(int)\n",
    "\n",
    "    # Price-based Features\n",
    "    df_copy[\"hl_range\"] = df_copy[\"High\"] - df_copy[\"Low\"]\n",
    "    df_copy[\"vol_change\"] = df_copy.groupby(\"Ticker\")[\"Volume\"].pct_change()\n",
    "    df_copy['oc_diff'] = df_copy['Close'] - df_copy['Open']\n",
    "    df_copy['gap'] = df_copy['Open'] - df_copy.groupby('Ticker')['Close'].shift(1)\n",
    "    \n",
    "    # Momentum Indicators (RSI, Bollinger, MACD)\n",
    "    df_copy[\"rsi\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].transform(lambda x: calc_rsi(x, 14))\n",
    "    b_range = 20\n",
    "    bollinger_ma = df_copy.groupby(\"Ticker\")[\"Adj Close\"].rolling(b_range).mean().reset_index(0, drop=True)\n",
    "    bollinger_std = df_copy.groupby(\"Ticker\")[\"Adj Close\"].rolling(b_range).std().reset_index(0, drop=True)\n",
    "    bollinger_upper = bollinger_ma + 2 * bollinger_std\n",
    "    bollinger_lower = bollinger_ma - 2 * bollinger_std\n",
    "    df_copy['bollinger_width'] = (bollinger_upper - bollinger_lower) / bollinger_ma\n",
    "\n",
    "    exp1 = df_copy.groupby(\"Ticker\")[\"Adj Close\"].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "    exp2 = df_copy.groupby(\"Ticker\")[\"Adj Close\"].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "    df_copy['macd'] = exp1 - exp2\n",
    "    df_copy['macd_signal'] = df_copy.groupby(\"Ticker\")['macd'].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n",
    "    df_copy['macd_hist'] = df_copy['macd'] - df_copy['macd_signal']\n",
    "    \n",
    "    df_copy.ta.ad(high=df_copy['High'], low=df_copy['Low'], close=df_copy['Close'], volume=df_copy['Volume'])\n",
    "    # Additional Technical Indicators using pandas-ta\n",
    "    df_copy.set_index(pd.MultiIndex.from_frame(df_copy[['Ticker', 'Date']]), inplace=True)\n",
    "    df_copy.ta.atr(length=14, append=True)\n",
    "    df_copy.ta.obv(append=True)\n",
    "    df_copy.ta.stoch(k=14, d=3, append=True)\n",
    "    df_copy.ta.adx(append=True)\n",
    "    df_copy.ta.willr(length=14, append=True)\n",
    "    df_copy.ta.willr(length=5, append=True)\n",
    "    df_copy.ta.cci(length=20, append=True)\n",
    "    df_copy.ta.cci(length=14, append=True)\n",
    "    df_copy.reset_index(inplace=True, drop=True)\n",
    "    df_copy.rename(columns={'ATRr_14': 'atr_14d', 'OBV': 'obv', 'WILLR_14': 'WR_14', 'WILLR_5': 'WR_5', 'CCI_20_0.015': 'CCI_20', 'CCI_14_0.015': 'CCI_14', 'ADL': 'ADL'}, inplace=True)\n",
    "    df_copy[\"atr_14d_norm\"] = df_copy[\"atr_14d\"] / df_copy[\"Adj Close\"]\n",
    "\n",
    "    # Target Variables (Future Returns)\n",
    "    df_copy[\"target_1d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].pct_change(1).shift(-1)\n",
    "    df_copy[\"target_5d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].pct_change(5).shift(-5)\n",
    "    df_copy[\"target_20d\"] = df_copy.groupby(\"Ticker\")[\"Adj Close\"].pct_change(20).shift(-20)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def engineer_index_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applies feature engineering steps to the S&P 500 index data.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"Date\"] = pd.to_datetime(df_copy[\"Date\"])\n",
    "    df_copy = df_copy.sort_values([\"Date\"])\n",
    "\n",
    "    # Returns, MAs, and Volatility\n",
    "    df_copy[\"sp500_return_1d\"] = df_copy[\"sp500_Adj Close\"].pct_change(1)\n",
    "    \n",
    "    df_copy[\"sp500_return_10d\"] = df_copy[\"sp500_Adj Close\"].pct_change(10)\n",
    "    df_copy[\"sp500_return_20d\"] = df_copy[\"sp500_Adj Close\"].pct_change(20)\n",
    "    df_copy[\"sp500_ma_10d\"] = df_copy[\"sp500_Adj Close\"].rolling(10).mean()\n",
    "    df_copy[\"sp500_ema_20d\"] = df_copy[\"sp500_Adj Close\"].ewm(adjust=False, span=20).mean()\n",
    "    df_copy[\"sp500_ema_30d\"] = df_copy[\"sp500_Adj Close\"].ewm(adjust=False, span=30).mean()\n",
    "    df_copy[\"sp500_ema_60d\"] = df_copy[\"sp500_Adj Close\"].ewm(adjust=False, span=60).mean()\n",
    "    df_copy[\"sp500_ema_90d\"] = df_copy[\"sp500_Adj Close\"].ewm(adjust=False, span=90).mean()\n",
    "    df_copy[\"sp500_rv_20d\"] = df_copy[\"sp500_return_1d\"].rolling(20).std()\n",
    "    df_copy[\"sp500_rv_40d\"] = df_copy[\"sp500_return_1d\"].rolling(40).std()\n",
    "    df_copy[\"sp500_rv_80d\"] = df_copy[\"sp500_return_1d\"].rolling(80).std()\n",
    "    df_copy[\"sp500_rv_120d\"] = df_copy[\"sp500_return_1d\"].rolling(120).std()\n",
    "\n",
    "    # Price-based and Momentum\n",
    "    df_copy[\"sp500_hl_range\"] = df_copy[\"sp500_High\"] - df_copy[\"sp500_Low\"]\n",
    "    df_copy[\"sp500_vol_change\"] = df_copy[\"sp500_Volume\"].pct_change()\n",
    "    df_copy['sp500_oc_diff'] = df_copy['sp500_Close'] - df_copy['sp500_Open']\n",
    "    df_copy['sp500_gap'] = df_copy['sp500_Open'] - df_copy['sp500_Close'].shift(1)\n",
    "    df_copy[\"sp500_rsi\"] = calc_rsi(df_copy[\"sp500_Adj Close\"], 14)\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df_copy[\"sp500_Adj Close\"].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df_copy[\"sp500_Adj Close\"].ewm(span=26, adjust=False).mean()\n",
    "    sp500_macd = exp1 - exp2\n",
    "    df_copy['sp500_macd_signal'] = sp500_macd.ewm(span=9, adjust=False).mean()\n",
    "    df_copy['sp500_macd_hist'] = sp500_macd - df_copy['sp500_macd_signal']\n",
    "    \n",
    "    # ATR & OBV for index\n",
    "    df_copy[\"sp500_atr_14d\"] = ta.atr(high=df_copy[\"sp500_High\"], low=df_copy[\"sp500_Low\"], close=df_copy[\"sp500_Adj Close\"], length=14)\n",
    "    df_copy[\"sp500_atr_14d_norm\"] = df_copy[\"sp500_atr_14d\"] / df_copy[\"sp500_Adj Close\"]\n",
    "    df_copy[\"sp500_obv\"] = ta.obv(close=df_copy[\"sp500_Adj Close\"], volume=df_copy[\"sp500_Volume\"])\n",
    "    \n",
    "    return df_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution Pipeline\n",
    "This section runs the complete data processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching S&P 500 tickers...\n",
      "Found 503 tickers.\n",
      "Fetching historical stock data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  10 of 10 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching S&P 500 index data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/var/folders/94/xf2q8_bs543d00kh0st2v5t80000gn/T/ipykernel_7625/4147691023.py:26: ChainedAssignmentError: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "When using the Copy-on-Write mode, such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_data['Sector'].fillna('Unknown', inplace=True)\n",
      "/var/folders/94/xf2q8_bs543d00kh0st2v5t80000gn/T/ipykernel_7625/4147691023.py:27: ChainedAssignmentError: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "When using the Copy-on-Write mode, such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_data['Industry'].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging company information...\n",
      "Engineering features for the S&P 500 index...\n",
      "Merging stock data with index features...\n",
      "Engineering features for individual stocks...\n",
      "Cleaning final dataset and selecting columns...\n",
      "Saving final data to ../data/processed/sp500.parquet...\n",
      "Pipeline finished successfully!\n",
      "Final DataFrame shape: (49513, 73)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to execute the data pipeline.\"\"\"\n",
    "    # --- Define file paths ---\n",
    "    output_dir = \"../data/processed/\"\n",
    "    company_info_path = \"../data/raw/sp500_companies.csv\" # Assuming this file exists\n",
    "    output_path = os.path.join(output_dir, \"sp500.parquet\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Fetch S&P 500 Tickers\n",
    "    print(\"Fetching S&P 500 tickers...\")\n",
    "    tickers_df = list_slickcharts_sp500()\n",
    "    tickers = list(tickers_df[\"Symbol\"])\n",
    "    print(f\"Found {len(tickers)} tickers.\")\n",
    "\n",
    "    # 2. Fetch Historical Data for Stocks and Index\n",
    "    print(\"Fetching historical stock data...\")\n",
    "    stock_data = fetch_sp500_stock_data(tickers[:10])\n",
    "    print(\"Fetching S&P 500 index data...\")\n",
    "    index_data = fetch_sp500_index_data()\n",
    "\n",
    "    # 3. Load and Merge Company Info\n",
    "    print(\"Loading and merging company information...\")\n",
    "    sp500_companies = pd.read_csv(company_info_path)\n",
    "    sp500_companies.rename(columns={\"Symbol\": \"Ticker\"}, inplace=True)\n",
    "    merged_data = pd.merge(stock_data, sp500_companies[[\"Ticker\", \"Sector\", \"Industry\"]], on=\"Ticker\", how=\"left\")\n",
    "    merged_data['Sector'].fillna('Unknown', inplace=True)\n",
    "    merged_data['Industry'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # 4. Feature Engineering\n",
    "    print(\"Engineering features for the S&P 500 index...\")\n",
    "    index_features = engineer_index_features(index_data)\n",
    "    \n",
    "    # 5. Merge stock data with index features\n",
    "    print(\"Merging stock data with index features...\")\n",
    "    merged_data.reset_index(inplace=True)\n",
    "    index_features.reset_index(inplace=True)\n",
    "    merged_data[\"Date\"] = pd.to_datetime(merged_data[\"Date\"])\n",
    "    index_features[\"Date\"] = pd.to_datetime(index_features[\"Date\"])\n",
    "    final_df = pd.merge(merged_data, index_features, on=\"Date\", how=\"left\")\n",
    "\n",
    "    print(\"Engineering features for individual stocks...\")\n",
    "    final_df = engineer_features(final_df)\n",
    "    \n",
    "    # 6. Final Cleaning and Column Selection\n",
    "    print(\"Cleaning final dataset and selecting columns...\")\n",
    "    # Replace inf values that may arise from pct_change with 0 or NaN\n",
    "    \n",
    "    final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    final_df.dropna(inplace=True)\n",
    "    \n",
    "    # Ensure all required final columns are present\n",
    "    final_columns = [\n",
    "        'Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
    "        'Sector', 'Industry', 'return_1d', 'return_5d', 'return_20d', 'ma_5d',\n",
    "        'ma_20d', 'ema_5d', 'ema_20d', 'rv_5d', 'rv_20d', 'week_of_year',\n",
    "        'month', 'quarter', 'hl_range', 'vol_change', 'oc_diff', 'gap', 'rsi',\n",
    "        'macd', 'macd_signal', 'macd_hist', 'target_1d', 'target_5d',\n",
    "        'target_20d', 'day_of_week', 'day_of_month', 'is_quarter_start',\n",
    "        'is_quarter_end', 'sp500_Adj Close', 'sp500_Volume', 'sp500_return_10d',\n",
    "        'sp500_return_20d', 'sp500_ma_10d', 'sp500_ema_20d', 'sp500_rv_20d',\n",
    "        'sp500_hl_range', 'sp500_vol_change', 'sp500_oc_diff', 'sp500_gap',\n",
    "        'sp500_rsi', 'sp500_macd_signal', 'sp500_macd_hist', 'bollinger_width',\n",
    "        'atr_14d', 'atr_14d_norm', 'obv', 'STOCHk_14_3_3', 'STOCHd_14_3_3',\n",
    "        'STOCHh_14_3_3', 'ADX_14', 'ADXR_14_2', 'DMP_14', 'DMN_14',\n",
    "        'sp500_ema_30d', 'sp500_ema_60d', 'sp500_ema_90d', 'WR_14', 'WR_5',\n",
    "        'CCI_20', 'CCI_14', 'ADL', 'sp500_rv_40d', 'sp500_rv_80d','ADL'\n",
    "        'sp500_rv_120d', 'sp500_atr_14d_norm', 'sp500_obv'\n",
    "    ]\n",
    "    # Select only the columns that exist in the dataframe to avoid errors\n",
    "    existing_final_columns = [col for col in final_columns if col in final_df.columns]\n",
    "    final_df = final_df[existing_final_columns]\n",
    "    \n",
    "    # 7. Save to Parquet\n",
    "    print(f\"Saving final data to {output_path}...\")\n",
    "    final_df.to_parquet(output_path, index=False)\n",
    "    print(\"Pipeline finished successfully!\")\n",
    "    print(f\"Final DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "# Run the main pipeline\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
